<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://timpcfan.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>TrystanLei</title>
    <link>https://timpcfan.github.io/</link>
    <description>积累点滴，汇聚成溪。</description>
    <language>zh-CN</language>
    <pubDate>Sat, 01 Oct 2022 05:39:01 GMT</pubDate>
    <lastBuildDate>Sat, 01 Oct 2022 05:39:01 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <category>随笔</category>
    <category>经验</category>
    <category>学习</category>
    <item>
      <title>2017/10/18 -「心」熬夜有感</title>
      <link>https://timpcfan.github.io/note/2017-10-18.html</link>
      <guid>https://timpcfan.github.io/note/2017-10-18.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2017/10/18 -「心」熬夜有感</source>
      <category>随笔</category>
      <pubDate>Wed, 18 Oct 2017 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><img src="https://images.unsplash.com/photo-1479267658415-ff274a213280?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=png&amp;ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwxOXx8bmlnaHR8ZW58MHx8fHwxNjM3MDY5MTUy&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1080" alt="image" loading="lazy"></p>
<p>嗯。。现在是凌晨五点，我还没睡着。</p>
<p>十一点上床，十二点睡觉。</p>
<p>睡前看了会儿手机，有点兴奋，睡不着。</p>
<p>努力睡着，挣扎着睡着，睡不着。</p>
<p>想着熬夜对身体不好，更拼命地睡着，更睡不着。</p>
<p>数绵羊？998、999、1000。好，两点了，睡不着。</p>
<p>想到以前背单词，背着背着就困了，于是开始背单词，背完单词，莫名地更加精神，三点了，睡不着。</p>
<p>越熬越害怕，怕今晚休息不够，明天没精力学习，心跳加速，睡不着。</p>
<p>四点钟，听到宿舍外清洁工阿姨已经开始工作了，我意识到这晚上已接近尾声。我渐渐开始不再焦虑了，睡不着的话，继续背单词也不错吧。今天没睡够，明天补上就好。</p>
<p>又打开扇贝单词直到现在。</p>
<p>五点钟，站在阳台，看见对面宿舍全部关着灯，听见树丛中虫子在鸣叫，我的内心感受着深夜的宁静。回想当初挣扎入睡，不由地觉得可笑。故觉得有必要记录一下，便打开博客记下此感。</p>
<p>嘛，其实很多事情都是这样的。</p>
<p>从一开始的 无知担心 到 痛苦焦虑 到最后 懂得了放下，才能得到解脱。</p>
<p>写完这篇以后已经五点四十七分了，决定去睡觉了。</p>
<p>TrystanLei</p>
<p>记于2017年10月18日凌晨</p>
]]></content:encoded>
      <enclosure url="https://images.unsplash.com/photo-1479267658415-ff274a213280?crop=entropy&cs=tinysrgb&fit=max&fm=png&ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwxOXx8bmlnaHR8ZW58MHx8fHwxNjM3MDY5MTUy&ixlib=rb-1.2.1&q=80&w=1080" type="image/"/>
    </item>
    <item>
      <title>2018/12/04 -「心」炒股的感悟</title>
      <link>https://timpcfan.github.io/note/2018-12-04.html</link>
      <guid>https://timpcfan.github.io/note/2018-12-04.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2018/12/04 -「心」炒股的感悟</source>
      <category>随笔</category>
      <pubDate>Tue, 04 Dec 2018 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><img src="https://images.unsplash.com/photo-1611974789855-9c2a0a7236a3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=png&amp;ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwxfHxzdG9ja3xlbnwwfHx8fDE2MzcwNzM5MjY&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1080" alt="image" loading="lazy"></p>
<p>太久没有写过博客了，今天趁G20中美和谈之后，美股大涨之际谈下最近所思所想。</p>
<p>先交代一下背景吧，最近股市波动，每个夜晚都让我心惊胆颤，心神不宁，无法集中注意力，这段时间里，我经过了多次的失误操作以后，总资产一度跌破了30%，在这迷茫之际，我决定将我的感悟写下。</p>
<p>这篇文章不是想谈我是怎么炒股亏钱的，而是想谈一个有关的感悟。</p>
<p>随着时代的进步，物质生活的丰富，人类可以做出的选择也是越来越多样化了，在这有多样选择的世界里，大家是感觉更幸福了呢，还是更焦虑了呢。</p>
<p>就我而言：多样的选择使人更焦虑。</p>
<p>为什么这么说呢？</p>
<p>这是我从亲身体会里总结出来的，先说个例子吧：</p>
<p>上小学中学的时候，必须要穿校服，没得选择，觉得穿校服是件很自然的事情，每天轻松愉快的很快就换好了衣服去上学了。</p>
<p>到大学以后，没有校服，要自己买衣服了，就多了选择。</p>
<p>牌子、颜色、尺寸、价格、质量都可以选，选来选去，费时费钱又费精力。</p>
<p>选择多了，人，也就变焦虑了。</p>
<p>这还只是买衣服罢，你还得选择每天要穿什么衣服，还要选择如何搭配，考虑出行的场合，与谁会面，考虑我是不是这周穿过了这套衣服，考虑会不会和别人撞衫，考虑太多太多。。。</p>
<p>多样的选择给生活增添了一份负担。</p>
<p>再举个股票的例子：</p>
<p>先要说句很有争议的话：相比炒美股，炒A股是幸福的。</p>
<p>你可能在第一时间就会反驳我说的这句话，心想你肯定是美股亏太多钱，觉得A股有涨跌幅限制就会亏少一点，却忘记美股是牛市，A股是熊市的事实。</p>
<p>我呢，当然是知道这一点的，但我想说的幸福并不是在哪好赚钱，在哪不好赚钱的问题，我比较的依据是对人的折磨程度。</p>
<p>在A股，一般的散户是只能做多的（这里说的是一般的散户啦，别计较什么融资什么的），并且只能T+1交易，也就是说你当天买入要次日后才能卖出。</p>
<p>炒A股时我是这样的：我看好一支股票，我<strong>唯一</strong>的选择就是要在什么价位上买多少股呢，我当天买进去以后，这天的交易就结束了，因为我<strong>无法再支配</strong>我买进去的这部分资金了，它是涨还是跌我都只能看着。</p>
<p>但在美股，一切就不一样了。</p>
<p>美股里可以做空和做多，可以轻易的融资上杠杆，最重要的是它可以T+0交易，也就是说你可以在当天买入当天卖出，反悔无数次（有代价的）。</p>
<p>炒美股时我是这样的：我看好一支股票，先选择在什么价位买多少股，买进去以后就一直盯着盘，因为我<strong>还有机会反悔</strong>啊。我买进去以后就看着，它一跌我就在想我是不是该提前抛掉止损。它一涨我又在想，它是不是已经涨到顶点了该抛掉止盈。每一次方向的变动都会在我的心里掀起不小的波澜，不断拍打我的意志，折磨我的内心。</p>
<p>在A股里，你一天的选择是<strong>有限</strong>的，你进行完当天的交易以后你就只能看着。而在美股里，你一天的选择是<strong>无限</strong>的，你可以<strong>不断地</strong>买进卖出买进卖出。美股给你更多的选择，同时给你带来了更多的焦虑与压力。</p>
<p>我不是想说哪个市场好哪个市场坏，我只是想表明焦虑的来源是选择的多样性造成的，越多的选择带来越大的焦虑。</p>
<p>人类的一大共同点就是害怕损失，害怕自己的利益没有最大化，这就能解释为什么选择使人焦虑，人需要做出选择，通常都是在几个对自己利益程度不同的选项中选择，而且一般都没有完美的选项（e.g., 这衣服超好看超舒服，但好贵啊。 这工作超轻松超适合我，但工资好低啊。诸如此类）。为了让自己的利益最大化，我们会不断的比较不同的选项，从不同选项中找到最优解，这个过程是很耗费精力的，选择的越久，耗费的精力越大，人也就会越来越焦虑，越来越烦躁。</p>
<p>焦虑影响人们做出理性的决定。</p>
<p>一旦被选择折磨久了，注定变得焦虑，在焦躁不安的情况下，人们的理性思维能力会下降，容易冲动，做出欠思考的决定（e.g., 你可能之前研究一个股票了很久决定要在这个股票跌到13刀再买入，但当你看到这股票势如破竹的涨到16刀的时候，冲动地放弃自己这么久的研究的成果，直接买入，结果追高了）。因此在有<strong>更多选择</strong>的美股市场里，你更容易变得焦虑，并更容易做出错误的操作。</p>
<p>所以说，当你要做出重大决定的时候，冷静下来思考一下：这个决定是不是一时冲动造成的，是非理性的呢？</p>
<p>在这越来越自由、物质种类越来越繁多的时代里，选择自然而然也就会变得很多，要如何在这个时代里做到不再焦虑，似乎是一件比较困难的事呢。</p>
<p>TrystanLei</p>
<p>于2018/12/4凌晨</p>
]]></content:encoded>
      <enclosure url="https://images.unsplash.com/photo-1611974789855-9c2a0a7236a3?crop=entropy&cs=tinysrgb&fit=max&fm=png&ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwxfHxzdG9ja3xlbnwwfHx8fDE2MzcwNzM5MjY&ixlib=rb-1.2.1&q=80&w=1080" type="image/"/>
    </item>
    <item>
      <title>2019/06/19 -「杂」记一次网站搬家</title>
      <link>https://timpcfan.github.io/note/2019-06-19.html</link>
      <guid>https://timpcfan.github.io/note/2019-06-19.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2019/06/19 -「杂」记一次网站搬家</source>
      <category>随笔</category>
      <pubDate>Wed, 19 Jun 2019 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><img src="https://images.unsplash.com/photo-1498050108023-c5249f4df085?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=png&amp;ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwzOXx8c2VydmVyfGVufDB8fHx8MTYzNzA3NjM3OA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1080" alt="image" loading="lazy"></p>
<p>今天又进行了一年一度的网站搬家。</p>
<p>为什么要网站搬家呢？</p>
<p>还不是因为要贪点小便宜，每年去找优惠价格的服务器，而这些服务器虽然价格便宜但是要续费就会比原来购买的价格贵上几倍甚至几十倍。</p>
<p>于是快要到期的时候，我通常都会重新去找优惠的服务器，然后把整个网站都搬过去。</p>
<p>最初这个网站是部署在阿里云ECS上的，由于学生优惠还算比较便宜，配置也很好：2G的内存，独立ip，独占的1M宽带。</p>
<p>一年后发现，百度云（不是网盘那个百度云）上面的虚拟主机太便宜了，当时是512m的内存，2M宽带，价格19.9一年。于是我毫不犹豫地将网站搬到了百度云上来。</p>
<p>又过了一年，也就是到今天，服务器快到期了，我发现如果要续费，那就要付20倍的价格，也就是399，这比我在阿里云上租服务器还要贵，果断放弃。再找其他优惠，发现重新去买活动的服务器会便宜很多，还是百度云的内存128m，2m宽带，20块钱一年。</p>
<p>配置稍微有点低，我以为是够用的，就买来试试，毕竟也不贵。买完部署完以后，发现128m的内存有点够呛，运行WordPress，如果只是浏览网页还好，要用什么附加的插件呀，或者甚至只是想更新一下WordPress的版本都会爆内存。</p>
<p>这也算是贪小便宜的后果吧，于是现在我把多余的插件什么的都关闭了，统计服务、缓存之类的功能都没有开，这才勉强能跑的起来。</p>
<p>现在又在考虑要不要把网站搬回阿里云ECS算了，学生优惠还有一年的时间，不过先用用看吧orz</p>
<p>ps：这次搬家完以后顺便启用了https，百度云上的免费https申请非常方便（不是广告啦）</p>
]]></content:encoded>
      <enclosure url="https://images.unsplash.com/photo-1498050108023-c5249f4df085?crop=entropy&cs=tinysrgb&fit=max&fm=png&ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwzOXx8c2VydmVyfGVufDB8fHx8MTYzNzA3NjM3OA&ixlib=rb-1.2.1&q=80&w=1080" type="image/"/>
    </item>
    <item>
      <title>2019/06/24 - 「水」用两只手表示1000多个数字</title>
      <link>https://timpcfan.github.io/note/2019-06-24.html</link>
      <guid>https://timpcfan.github.io/note/2019-06-24.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2019/06/24 - 「水」用两只手表示1000多个数字</source>
      <category>随笔</category>
      <pubDate>Mon, 24 Jun 2019 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><img src="https://images.unsplash.com/photo-1611957082126-061f655ef1fb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=png&amp;ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwyfHxjb3VudGluZ3xlbnwwfHx8fDE2MzcwNzQwMTY&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1080" alt="image" loading="lazy"></p>
<p>今天吃完饭看群友聊天，他们在聊做菜的话题，在讨论会做什么菜。</p>
<p>阿花表示「自己做的菜用两只手就能数得过来」</p>
<p>这时数字怪回了一句「五进制？」</p>
<p>这突然间引发了我思考，以前基本上只用手表示0至9这10个数字，因为这些最常用到。如果要表示超过10的数，通常需要两只手来表示。一只手作为十位，一只手作为个位，这样通过两只手组合就可以表示0~99的100个数字。</p>
<p>但这就是极限了吗？</p>
<p>当然不是，一只手能表示10个数字是因为我们只规定了0~9的手势，我们只用规定更多的手势当然就可以表示更多的数了。</p>
<p>然而这大大增加了记忆的负担，并且还要花心思如何去设计更多的手势。</p>
<p>数字怪说的「五进制」启发了我，如果在手上用五进制来进行数数的话，一只手当做低位，另一只手当做高位，低位的手数满了5以后，向高位的手进一位。这样一共能表示5^2=25个数字。</p>
<p>虽然25个数字没有上面100个数字多，但是启发了我，可以通过变换进制来数更多的数字。</p>
<p>实际上，我们很容易联想到，手指的伸出与否表示两个状态，如果使用二进制来表示，刚好可以让手指伸出表示1，收起来表示0。这样，我们一般有10个手指，就可以通过二进制表示2^10=1024个数字。这样就能充分的利用每个手指所提供的表示能力了。</p>
<p>更进一步：我们甚至可以可以使用三进制，一根手指可以表示三个状态，如：伸直，弯曲，收起，分别表示2、1、0，这样十个手指一共可以组合成3^10=59049个状态，也就是说可以表示近6万个数字！！是不是有点厉害呢？</p>
<p>然而没人会这么无聊用三进制手势来表示数字的吧：）</p>
]]></content:encoded>
      <enclosure url="https://images.unsplash.com/photo-1611957082126-061f655ef1fb?crop=entropy&cs=tinysrgb&fit=max&fm=png&ixid=MnwxNDIyNzR8MHwxfHNlYXJjaHwyfHxjb3VudGluZ3xlbnwwfHx8fDE2MzcwNzQwMTY&ixlib=rb-1.2.1&q=80&w=1080" type="image/"/>
    </item>
    <item>
      <title>2020/06/30 - 关于使用Anki的一些经验</title>
      <link>https://timpcfan.github.io/note/2020-06-30-how_to_use_anki.html</link>
      <guid>https://timpcfan.github.io/note/2020-06-30-how_to_use_anki.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2020/06/30 - 关于使用Anki的一些经验</source>
      <category>经验</category>
      <pubDate>Tue, 30 Jun 2020 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<blockquote>
<p>本文是写给我朋友用于备考研究生的，分享了我考研时期使用Anki这个强大的记忆工具的一些心得体会，现在分享给大家。</p>
</blockquote>
<ul>
<li>使用OmniFocus（或者任意的任务管理软件）缓存新卡片
<ul>
<li>在看视频或做题看到重点内容，先截图，写上相关内容，扔进OF的收件箱中</li>
<li>避免边学习边做卡片打断学习</li>
<li>每天晚上整理收集的卡片，加入进anki</li>
</ul>
</li>
<li>卡片尽量自己制作，除非万不得已
<ul>
<li>制作卡片是学习的过程</li>
<li>自己的话更容易理解</li>
<li>除非是类似政治选择题这样的标准题库，都应该自己制作</li>
</ul>
</li>
<li>每天一定要完成当日任务
<ul>
<li>anki任务尽量不要拖到第二天</li>
</ul>
</li>
<li>关于时间间隔的设置
<ul>
<li>对于政治这类题，可以把间隔设置很大，避免天天都出现，影响效率</li>
<li>根据考试剩余的天数可以适当调整最大间隔</li>
</ul>
</li>
<li>关于卡片模板
<ul>
<li>卡片模板在精不再多</li>
<li>我选择和改造四种模板
<ul>
<li>问答题（最基础，最常用）</li>
<li>填空题（基础）</li>
<li>翻转题（少用）</li>
<li>多选题（对于政治题可用）
<ul>
<li>如果你需要我的政治复习题题库，可以在文章下方找到。</li>
</ul>
</li>
</ul>
</li>
<li>这些卡片模板是根据费曼学习法来制作的
<ul>
<li>每个卡片都包含附加的几个部分
<ul>
<li>评论：使用自己的话，简要说明知识点</li>
<li>举例：自己尝试着针对这个知识点举例</li>
<li>拓展：随便放其他东西</li>
</ul>
</li>
<li>关于费曼学习法
<ul>
<li>一个公认的最有效的学习方法</li>
<li>核心是：<strong>说人话</strong></li>
<li>如果你能将一个知识点用自己的话（简单的语言，而不是术语）讲给80岁的老人听，让他能听懂，那才证明你真正掌握了这个知识（当然80岁这里有点夸张，就是强调外行人也能听懂）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>关于插件
<ul>
<li>Review Heatmap：复习热度图</li>
<li>Quick note and deck buttons：快速切换笔记类型的按钮</li>
<li>ImageResizer：自动更改图片的大小</li>
<li>Image Occlusion Enhanced：【神器】使用图片蒙版来批量制作卡片，类似于MarginNote的复习功能</li>
<li>Fast Word Query：批量查询单词，用于卡片制作</li>
<li>Speed Focus Mode：超时自动按不知道，刷单词非常酸爽</li>
<li>CrowdAnki JSON exportimport：将词库导出json格式备份，或修改</li>
</ul>
</li>
<li>如果想深入使用anki，推荐看一下anki的手册
<ul>
<li><a href="http://www.ankichina.net/manual/anki" target="_blank" rel="noopener noreferrer">http://www.ankichina.net/manual/anki</a></li>
</ul>
</li>
</ul>
<hr>
<h1 id="附录" tabindex="-1"> 附录</h1>
<p>四种卡片模板：</p>
<p><a href="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/AE74FA64-342F-4890-ACE3-298A06D6D7AC_2/XDP4eQPvW4ZYWyuOdXGV85eKv6Kvav3TfY4NfnMSZeIz/AE74FA64-342F-4890-ACE3-298A06D6D7AC_2.apkg" target="_blank" rel="noopener noreferrer">四种卡片模板.apkg</a></p>
<p>政治记忆库（需要使用CrowdAnki插件导入，需重置学习记录）：</p>
<p><a href="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/CE0AF409-2EB6-4B72-8FE3-22C4E8CD2D1B_2/Rtry3Wh0tTwiMceIWo9QzstrBbGULuUcKemFyyjGdFwz/CE0AF409-2EB6-4B72-8FE3-22C4E8CD2D1B_2.zip" target="_blank" rel="noopener noreferrer">政治.zip</a></p>
<p>政治刷题设置：</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/C06544E5-BEC7-4539-AE98-EB542196C0F6_2/N9UIxbvWxpsu4GprcZbWj2qfjIxFgDqDcLn7zicn1Coz/9598D7B7-396E-495D-84A7-8CE8FAEF53E5.png" alt="9598D7B7-396E-495D-84A7-8CE8FAEF53E5.png" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/39C7C5F7-ED04-470A-9130-E91B1B49056F_2/3AbVmcE6JA3TSkHKmNm8x7ksdUSliXKce6774NLA3T8z/B23CB780-7313-4409-91CD-65BDFF6A0C40.png" alt="B23CB780-7313-4409-91CD-65BDFF6A0C40.png" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/29085BC5-4178-41F4-97D7-110B5E48BE3F_2/ekGXXkTkIBD2G8nZSA8wpYS2A4MJWfH26mO8yCS1xx0z/C06DD0E1-7B7C-4560-946E-43D530E353E0.png" alt="C06DD0E1-7B7C-4560-946E-43D530E353E0.png" loading="lazy"></p>
<p>密集复习型（数学难点用）：</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/2B597463-D668-46D7-8912-88BDE9BEF66C_2/1DAjUZ0ibLPKiOLDirUOx5MrJtbitmO4RxIDr4VdjLQz/7CF1C6AF-3EB4-4417-83E6-9D1A1E41FA6E.png" alt="7CF1C6AF-3EB4-4417-83E6-9D1A1E41FA6E.png" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/4CB7BE65-FB54-427F-AAA3-8F55EEA82781_2/axawuvXtJxZZ6J87ED1NM5VOffAwuck9uUVxpr03Jn8z/50673115-A046-4B63-8322-8FB8559FDD21.png" alt="50673115-A046-4B63-8322-8FB8559FDD21.png" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/77D4C1AC-3F4E-4CB8-B7FE-AA422E68C43C_2/UkxC3IySJ0inN0PIGMsjfNfm1jlJg9DuyLkr7C0yYGcz/A28C501C-8E4A-4923-9B5C-76C0BC2940C7.png" alt="A28C501C-8E4A-4923-9B5C-76C0BC2940C7.png" loading="lazy"></p>
<p>关于各个参数的具体含义参见：<a href="http://www.ankichina.net/manual/anki" target="_blank" rel="noopener noreferrer">http://www.ankichina.net/manual/anki</a></p>
<p>我用到的所有插件：</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/96B85C97-AEED-4EB5-962E-05F51F5E7A18_2/KaTA5l3pTycCNw8RuldFCodO8I3vGDZ8lKvSCT6uVlcz/E6BA1210-D411-44E2-AC3E-8E468E75C98A.png" alt="E6BA1210-D411-44E2-AC3E-8E468E75C98A.png" loading="lazy"></p>
]]></content:encoded>
      <enclosure url="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/C06544E5-BEC7-4539-AE98-EB542196C0F6_2/N9UIxbvWxpsu4GprcZbWj2qfjIxFgDqDcLn7zicn1Coz/9598D7B7-396E-495D-84A7-8CE8FAEF53E5.png" type="image/png"/>
    </item>
    <item>
      <title>2020/07/31 - 「心」记近日微妙的情绪</title>
      <link>https://timpcfan.github.io/note/2020-07-31.html</link>
      <guid>https://timpcfan.github.io/note/2020-07-31.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2020/07/31 - 「心」记近日微妙的情绪</source>
      <category>随笔</category>
      <pubDate>Fri, 31 Jul 2020 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/DF5F0C4E-14AC-4CD8-9A9C-6C9F82F9ADCA_3" alt="image" loading="lazy"></p>
<div><p>注意</p>
<p>Warning：本文可能存在略微过激情感。</p>
</div>
<p>最近有一种情绪，特别的复杂微妙，喜欢一个事物，却不敢过分去关注它。可能是怕看到一些不好的东西而影响我对这个事物的喜爱（而这不好的东西可能根本就不存在）。又或者是怕过分地关注会让我产生厌倦感，我不愿意去想象将来我会有不喜欢这个事物的可能性。尝试保持一定的距离，或许是能维持这份感情最好的方式（？）</p>
<p>其实我说的就是<a href="https://himehina.jp" target="_blank" rel="noopener noreferrer">姬雏</a>（一个日本的虚拟偶像vtuber组合）了，我真的是超级喜欢她们，我怕自己会对她们产生任何的负面情绪，不敢相信自己会有不喜欢她们的一天。然而这都是真实可能发生的，就像我之前单推阿夸mea犬山信姬一样，后面不还是抛弃了她们。我不愿意这种事情在hh身上再发生。</p>
<p>我常常会因为她们的努力得不到更多人的赏识而感到悲伤，说她们多么多么努力什么的，然而事实上其他所有vtb，所有人，有谁不努力呢？所以说，以后我不要再说什么努力不努力了，大家都是努力的，对比努力只会给喜欢的人招黑罢。</p>
<p>我也不想去考据姬雏了，只是想单纯地爱着她们就好了，这种心情，复杂又简单。</p>
<p>有好长时间没有听《姬雏鸟》了，之前在mv出之前可能循环了上百次了，之后却不情愿去听了？为什么呢？甚至没有第一时间在虾米或网易云音乐平台刷这首歌的播放数。我觉得我是怕看到这些平台上面的评论，怕看到一些不好的评论，如果我不去看的话，我就可以当做不存在。可能就是因为这个原因，我才至今没有去平台上听《姬雏鸟》吧。不愿意接受任何对于我喜欢的事物的负面观点，这可能也是病吧。</p>
<p>似乎开始有点了解饭圈的那些人了，也许我已经变成他们一样了（？）有点可悲不是吗？</p>
<hr>
<p>2020/7/31</p>
<p>TrystanLei</p>
]]></content:encoded>
      <enclosure url="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/DF5F0C4E-14AC-4CD8-9A9C-6C9F82F9ADCA_3" type="image/"/>
    </item>
    <item>
      <title>2021/05/02 - 武汉的天气</title>
      <link>https://timpcfan.github.io/note/2021-05-02-seasons_in_wuhan.html</link>
      <guid>https://timpcfan.github.io/note/2021-05-02-seasons_in_wuhan.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2021/05/02 - 武汉的天气</source>
      <category>随笔</category>
      <pubDate>Sun, 02 May 2021 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>我是一个广东人，第一次离开广东，在武汉度过了一个春天。</p>
<p>来武汉之前，就听说了这里的天气，冬天极冷，夏天极热，四季分明。去年九月份来的武汉，父亲问我，适应了吗？但我还没呆够一整年，还没完整经历武汉的四季，说不上适应。</p>
<p>初来武汉，正值夏秋之交，这里和广州的天气不相上下，潮湿、炎热，同一个太阳，同一个蒸笼。</p>
<p>11月17日，迎来了入秋的第一次落叶，落叶漫天飞舞，地上和天空都被落叶染成了一种颜色。第一次见到这样的场景，非常震撼，这是在南方看不到的，广州虽然也会落叶，但可能是春天才零零星星委屈吧唧地飘落，没有武汉落叶落的酣畅淋漓，洒脱自如。</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/8B42D543-7232-460A-8262-CBA968A9DB54_2/Z8fQWSvxcgWswz4D2K8iPYXULKxRb7kigdu5Gr7FutEz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/C909FE34-03F6-4C41-9FD6-AD8CE816BDD9_2/TNhS8ONgcyXSToExtQxSHEpcTwnpgE4plQWgzxuy0q4z/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/13F1470C-6989-43EF-9483-36089692712C_2/KoDAsym3cJodBKhgpiDYogZ05zrBDz04k0xsg08GPYoz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p>12月13日，迎来了入冬的第一场飘雪，虽然雪花不大，也没有积雪，不过让我这个23岁大却还没见过下雪的人像小孩子一样激动。白色的雪花点缀着树枝，让人能暂时忘却世间的一切不快。</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/77FAC2C1-2A30-47EE-BDD8-9A8A2A491723_2/AvFmvP1ix0ghYBZ9ypHtia4W9Tvwk6Sr6yOR8XX0zQgz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/06579238-CF1F-4882-8000-6E7D42D511D7_2/9DYqsDmUwuu2u7FvxGit866C1HbXyQaKbwMyx3ymIjYz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/CE1EA823-E7D7-4DAD-ACDE-42A56C455D3B_2/IThMMHGztiGX0Hx2HdrgyPz1t4Hw1RSexzJj4bKzBsEz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p>武汉的秋天、冬天都经历过了，放假回来已是春天。</p>
<p>春天依旧很冷，穿了两个月的大棉袄，气温的回升似乎总是被某种来自西伯利亚的神秘力量给阻止着，每当我打算把棉袄收起来的时候，过不了多久就能让我打消这个念头。武汉的春天是寒冷的。</p>
<p>上个学期的一场车祸，让我爱上了走路这种出行方式，每天早上走到实验室或教学楼，下午再走回宿舍，每天来回步行一两个小时。走路，可以让我放慢生活的节奏，可以让我有更多的时间暴露在武汉美好的春色中，感受春风拂过发梢的轻柔，感受春日照在肌肤上的温暖，感受细雨来带的一丝凉意，感受万物复苏的躁动。武汉的春天是美好的。</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/F040238D-10D4-4C80-8CF3-8FFA0CD3422D_2/ixVgdAwkPFZyTJcf5q70RvBTNl0G6xmqaB0q4KGZXeAz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/594F97B2-58F9-4D4B-B190-41A88A72AD6E_2/pE9EkJvdgxeVAtlW4ZNhGJXAHcFxxaEr5xgTibQSkWoz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/29EC016A-14F5-4D07-840D-5CD79F41BFB3_2/DTv8mysuw4vqety4u23oLE0iQwxo7aKcsHKo2QY5aaIz/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p>但武汉的春天却不是完美的。天气渐渐转热，风也渐渐大起来了。大风吹舞着梧桐树和柳树，将他们的种子吹向大地。听起来很美好，却给我带来极大的困扰。原本喜欢走路的我，因为不希望被梧桐树飘舞的飞刺伤眼睛而选择尽量呆在宿舍。果然，再有诗意的事物，也不是全是好处，任何事物都有优点有缺点，武汉的春天也一样，并不是完美的，但却是值得让我感受和欣赏的。</p>
<p>TrystanLei</p>
<p>2021年5月2日</p>
]]></content:encoded>
      <enclosure url="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/2A81BD11-0992-42A2-B375-74171A0B4B65/8B42D543-7232-460A-8262-CBA968A9DB54_2/Z8fQWSvxcgWswz4D2K8iPYXULKxRb7kigdu5Gr7FutEz/Image.jpeg" type="image/jpeg"/>
    </item>
    <item>
      <title>2021/08/17 - 记更换笔记软件为Craft</title>
      <link>https://timpcfan.github.io/note/2021-08-17-replace_notes_software_to_craft.html</link>
      <guid>https://timpcfan.github.io/note/2021-08-17-replace_notes_software_to_craft.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2021/08/17 - 记更换笔记软件为Craft</source>
      <category>随笔</category>
      <pubDate>Tue, 17 Aug 2021 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>这几天在B站上看大耳朵TV的时候，看到up分享了他最近在使用的一个笔记软件——Craft，我看到这软件，就深深被它的颜值所吸引，马上下下来尝试了一下，真的非常丝滑，体验极佳，与我长期使用的功能越来越臃肿的印象笔记一对比，高下立判。于是就打算将笔记软件换成Craft。</p>
<p>其实最吸引我的就是它能成块整理笔记的功能，并使用块链接，在笔记之间导航，感觉对于建立知识网络来说非常的方便。其实，先前尝试过的在国外很火的Notion也有这样的功能，而且更加强大，但是由于在国内访问不方便，我就没有用它。</p>
<p>可能是最近比较关心笔记软件，B站今天给我推了一个介绍wolai的视频，是另一款笔记软件，一个本地化的Notion，比较吸引我的一点是它做了很多本地化的优化，比如可以使用中文首字母来输入指令，这对我们中国人是很友好的。我曾多次产生要不要将它替换Craft和印象笔记，作为我的主要笔记软件，但是思前想后，最终决定放弃。</p>
<p>放弃wolai的理由如下：</p>
<ul>
<li>功能太过繁杂，这是优点同时也是缺点</li>
<li>是一个webapp，这使得使用体验较差，也是我放弃它的最终原因</li>
</ul>
<p>作为中文版的Notion，它功能非常之多，与之伴随而来的是学习成本的增加，当你在使用的时候你需要消耗更多的精力去关注工具本身。这一点是我所不希望的。</p>
<blockquote>
<p>笔记软件只是一种方法，如果在方法上花费过多的精力，而不是在目的本身，这就本末倒置了。</p>
</blockquote>
<p>另外一点，这是一个webapp，而不是原生应用。现在的软件，为了降低开发成本，并快速适应多个不同平台，都倾向于开发webapp。特别是国产软件，基本上全是webapp，原生应用少之又少。让我放弃wolai的最后一根稻草，就是这个软件是webapp，这使得它跟原生应用Craft对比下相形见绌。虽然说在功能方面上，wolai完胜Craft，但是在功能和简洁的trade off上，优雅的Craft更能赢得我的心。最终还是选择了Craft。</p>
<p>另外，暂时还不打算放弃使用印象笔记，毕竟笔记太多也懒得迁移了，每次用到某个笔记再顺便迁移一下。当作一个冷备份来用就好了。</p>
<hr>
<p>2022/4/7更新：发现Notion居然没被墙，被其数据库功能给惊艳到了，还是将主力笔记软件换成了Notion。。</p>
]]></content:encoded>
    </item>
    <item>
      <title>2021/11/16 - 记重新建站</title>
      <link>https://timpcfan.github.io/note/2021-11-16-rebuild_the_site.html</link>
      <guid>https://timpcfan.github.io/note/2021-11-16-rebuild_the_site.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2021/11/16 - 记重新建站</source>
      <category>随笔</category>
      <pubDate>Tue, 16 Nov 2021 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>上周，无意间看到<a href="https://zuolan.me" target="_blank" rel="noopener noreferrer">左蓝大佬的一篇文章</a>，教你如何使用Craft笔记软件来建立个人网站，看得我跃跃欲试，于是就有了这个网站。</p>
<p>其实在几年前，我是有一个个人网站的，虽然没有发布几篇文章，但是有模有样地整了一个，算是经营了一段时间吧。后来，因为网站闲置了很久，再加上阿里云教育优惠便宜买的服务器到期了，被迫歇业关门，关停了网站。</p>
<p>如今，有如此简单又廉价的方案建站，何乐而不为呢？</p>
<p>为什么要建立一个个人网站呢？人作为一个会思考的动物，总是在想东西，想的多了就需要输出，不然真的是憋的慌。找人倾诉吧，不是所有人都愿意听，每个人都有自己的见解，还是作为文字记录下来更加舒坦。我想这也很多人愿意写博客的原因吧。另外，个人网站不像朋友圈，并不会主动推送到朋友的timeline，只有真正在乎你的人才会去主动访问你的网站，这样的人少之又少，所以呢，就可以更加随心所欲地写一些不会在朋友圈中发表的内容。</p>
<p>这次，我的网站又能坚持多久呢？</p>
<hr>
<p>TrystanLei</p>
<p>2021/11/16晚写于杭州出租屋内</p>
]]></content:encoded>
    </item>
    <item>
      <title>2021/12/30 - Euler框架源码解读：深入探究NodeEstimator</title>
      <link>https://timpcfan.github.io/note/2021-12-30-dive_into_nodeestimator.html</link>
      <guid>https://timpcfan.github.io/note/2021-12-30-dive_into_nodeestimator.html</guid>
      <source url="https://timpcfan.github.io/rss.xml">2021/12/30 - Euler框架源码解读：深入探究NodeEstimator</source>
      <category>学习</category>
      <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="euler框架源码解读-深入探究nodeestimator" tabindex="-1"> Euler框架源码解读：深入探究NodeEstimator</h2>
<blockquote>
<p>本文一探Euler的图采样流程，从代码层面深入分析NodeEstimator。第一次写这种源码分析的文章，由于调用栈略深，且鄙人文字功底薄弱，致使行文稍显繁琐，还望读者见谅。</p>
</blockquote>
<h2 id="estimator的创建" tabindex="-1"> Estimator的创建</h2>
<p>NodeEstimator在训练时，会创建一个tf.estimator.Estimator来进行训练，我们首先来看一下tf.estimator.Estimator是如何创建的。</p>
<p>首先，NodeEstimator为BaseEstimator的子类，主要是重写了</p>
<ul>
<li>train_input_fn()：直接返回batch_size，会被get_train_from_input() 接收</li>
<li>get_train_from_input()：采样inputs个节点，返回n_id（1-D tensor of nodes）</li>
</ul>
<div><pre><code><span>class</span> <span>NodeEstimator</span><span>(</span>BaseEstimator<span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> model_class<span>,</span> params<span>,</span> run_config<span>)</span><span>:</span>
        <span>super</span><span>(</span>NodeEstimator<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>model_class<span>,</span> params<span>,</span> run_config<span>)</span>

    <span>def</span> <span>get_train_from_input</span><span>(</span>self<span>,</span> inputs<span>,</span> params<span>)</span><span>:</span>
        result <span>=</span> tf_euler<span>.</span>sample_node<span>(</span>inputs<span>,</span> params<span>[</span><span>'train_node_type'</span><span>]</span><span>)</span> <span># 访问euler服务器进行采样</span>
        result<span>.</span>set_shape<span>(</span><span>[</span>self<span>.</span>params<span>[</span><span>'batch_size'</span><span>]</span><span>]</span><span>)</span> 
        <span># set_shape用于提供额外shape信息，因为不能通过计算图来infer</span>
        <span>return</span> result

    <span>def</span> <span>train_input_fn</span><span>(</span>self<span>)</span><span>:</span>
        <span>return</span> self<span>.</span>params<span>[</span><span>'batch_size'</span><span>]</span>

    <span>.</span><span>.</span><span>.</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>再回到父类 BaseEstimator，在训练时调用 train() 方法，该方法会创建一个 tf.estimator.Estimator，并使用其进行训练。</p>
<p>这里有几个重要的函数需要关注：</p>
<ul>
<li>input_fn：用于模型数据的提供，这里就是前文提到<a href="craftdocs://open?blockId=D0ADA26B-9672-4CA3-A399-CA6F5BB180EC&amp;spaceId=4725d35f-0536-9d85-0a69-346665ba7ebe">train_input_fn()</a>，直接返回batch_size</li>
<li>model_fn：用于模型的训练，返回的是包装好训练逻辑的tf.estimator.EstimatorSpec，这里传入的是 BaseEstimator._model_fn()</li>
</ul>
<div><pre><code><span>class</span> <span>BaseEstimator</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>.</span><span>.</span><span>.</span>
    <span>def</span> <span>train</span><span>(</span>self<span>)</span><span>:</span>
        estimator <span>=</span> tf<span>.</span>estimator<span>.</span>Estimator<span>(</span>
                model_fn<span>=</span>self<span>.</span>_model_fn<span>,</span>
                params<span>=</span>self<span>.</span>params<span>,</span>
                config<span>=</span>self<span>.</span>run_config<span>,</span>
                model_dir<span>=</span>self<span>.</span>params<span>[</span><span>'model_dir'</span><span>]</span><span>)</span>

        <span>if</span> self<span>.</span>profiling<span>:</span>
            hooks <span>=</span> <span>[</span>tf<span>.</span>train<span>.</span>ProfilerHook<span>(</span><span>50</span><span>,</span> output_dir<span>=</span><span>"prof_dir"</span><span>)</span><span>]</span>
        <span>else</span><span>:</span>
            hooks <span>=</span> <span>[</span><span>]</span>
        <span>print</span> <span>(</span>self<span>.</span>profiling<span>,</span> hooks<span>)</span>
        total_step <span>=</span> <span>None</span>
        <span>try</span><span>:</span>
            total_step <span>=</span> self<span>.</span>params<span>[</span><span>'total_step'</span><span>]</span>
        <span>except</span><span>:</span>
            total_step <span>=</span> <span>None</span>
        estimator<span>.</span>train<span>(</span>input_fn<span>=</span>self<span>.</span>train_input_fn<span>,</span>
                        hooks<span>=</span>hooks<span>,</span>
                        <span>#steps=self.params['total_step'])</span>
                        steps<span>=</span>total_step<span>)</span>
    <span>.</span><span>.</span><span>.</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>继续看 BaseEstimator._model_fn()，他是 tf.estimator.Estimator 输入的 model_fn，其签名固定为 features, labels, mode, params：</p>
<ul>
<li>features：input_fn 返回的第一项</li>
<li>labels：input_fn 返回的第二项（这里没有）</li>
<li>mode：train or eval</li>
<li>params：一个装有超参数的dict</li>
<li>返回 tf.estimator.EstimatorSpec（指明了如何训练）</li>
</ul>
<div><pre><code><span>class</span> <span>BaseEstimator</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>.</span><span>.</span><span>.</span>
    <span>def</span> <span>_model_fn</span><span>(</span>self<span>,</span> features<span>,</span> mode<span>,</span> params<span>)</span><span>:</span>
        model <span>=</span> self<span>.</span>model_function
        <span>if</span> mode <span>==</span> tf<span>.</span>estimator<span>.</span>ModeKeys<span>.</span>TRAIN<span>:</span>
            spec <span>=</span> self<span>.</span>train_model_init<span>(</span>model<span>,</span> features<span>,</span> mode<span>,</span> params<span>)</span>
        <span>elif</span> mode <span>==</span> tf<span>.</span>estimator<span>.</span>ModeKeys<span>.</span>EVAL<span>:</span>
            spec <span>=</span> self<span>.</span>evaluate_model_init<span>(</span>model<span>,</span> features<span>,</span> mode<span>,</span> params<span>)</span>
        <span>else</span><span>:</span>
            spec <span>=</span> self<span>.</span>infer_model_init<span>(</span>model<span>,</span> features<span>,</span> mode<span>,</span> params<span>)</span>
        <span>return</span> spec
    <span>.</span><span>.</span><span>.</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>可以看到，在训练时调用了 BaseEstimator.train_model_init()</p>
<div><pre><code><span>class</span> <span>BaseEstimator</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>.</span><span>.</span><span>.</span>
    <span>def</span> <span>train_model_init</span><span>(</span>self<span>,</span> model<span>,</span> features<span>,</span> mode<span>,</span> params<span>)</span><span>:</span>
        source <span>=</span> self<span>.</span>get_train_from_input<span>(</span>features<span>,</span> params<span>)</span>  <span># 输入 features=batch_size 输出 n_id</span>
        _<span>,</span> loss<span>,</span> metric_name<span>,</span> metric <span>=</span> model<span>(</span>source<span>)</span> <span># 输入 n_id 输出 loss（采样在内部进行）</span>
        global_step <span>=</span> tf<span>.</span>train<span>.</span>get_or_create_global_step<span>(</span><span>)</span>
        optimizer <span>=</span> tf_euler<span>.</span>utils<span>.</span>optimizers<span>.</span>get<span>(</span>
                             params<span>.</span>get<span>(</span><span>'optimizer'</span><span>,</span> <span>'adam'</span><span>)</span><span>)</span><span>(</span>
                             params<span>.</span>get<span>(</span><span>'learning_rate'</span><span>,</span> <span>0.001</span><span>)</span><span>)</span>
        train_op <span>=</span> optimizer<span>.</span>minimize<span>(</span>loss<span>,</span> global_step<span>)</span>
        hooks <span>=</span> <span>[</span><span>]</span>
        tensor_to_log <span>=</span> <span>{</span><span>'step'</span><span>:</span> global_step<span>,</span>
                         <span>'loss'</span><span>:</span> loss<span>,</span>
                         metric_name<span>:</span> metric<span>}</span>
        hooks<span>.</span>append<span>(</span>
                tf<span>.</span>train<span>.</span>LoggingTensorHook<span>(</span>
                    tensor_to_log<span>,</span> every_n_iter<span>=</span>params<span>.</span>get<span>(</span><span>'log_steps'</span><span>,</span> <span>100</span><span>)</span><span>)</span><span>)</span>
        spec <span>=</span> tf<span>.</span>estimator<span>.</span>EstimatorSpec<span>(</span>mode<span>=</span>mode<span>,</span>
                                          loss<span>=</span>loss<span>,</span>
                                          train_op<span>=</span>train_op<span>,</span>
                                          training_hooks<span>=</span>hooks<span>)</span>
        <span>return</span> spec
    <span>.</span><span>.</span><span>.</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>这里才是主要配置训练时Estimator的行为的地方：</p>
<ol>
<li>使用<a href="craftdocs://open?blockId=2DD583FB-B6B5-418A-BCBD-4351E60DDE1C&amp;spaceId=4725d35f-0536-9d85-0a69-346665ba7ebe">get_train_from_input()</a>来获取一个mini-batch的节点id</li>
<li>将节点id输入模型model，得到损失函数</li>
<li>生成训练op，以及一些辅助的指标</li>
<li>创建tf.estimator.EstimatorSpec 并返回</li>
</ol>
<h2 id="消息传递流程" tabindex="-1"> 消息传递流程</h2>
<p>其实本文到这里就应该结束了，但是我还想要知道模型是如何采样的，消息是如何传递的，model里面具体做了什么，于是我们将NodeEstimator应用在一个具体的例子中，看看具体会发生什么。</p>
<p>以下是Euler官方的一个<a href="https://github.com/alibaba/euler/tree/master/examples/graphsage" target="_blank" rel="noopener noreferrer">例子</a>的简化版：</p>
<div><pre><code><span>from</span> __future__ <span>import</span> absolute_import
<span>from</span> __future__ <span>import</span> division
<span>from</span> __future__ <span>import</span> print_function

<span>import</span> tensorflow <span>as</span> tf
<span>import</span> tf_euler

<span>from</span> euler_estimator <span>import</span> NodeEstimator
<span>from</span> graphsage <span>import</span> SupervisedGraphSage

config <span>=</span> tf<span>.</span>ConfigProto<span>(</span><span>)</span>
config<span>.</span>gpu_options<span>.</span>allow_growth <span>=</span> <span>True</span>

dataset <span>=</span> <span>'cora'</span>
hidden_dim <span>=</span> <span>32</span>
layers <span>=</span> <span>2</span>
fanouts <span>=</span> <span>[</span><span>10</span><span>,</span> <span>10</span><span>]</span>
batch_size <span>=</span> <span>32</span>
num_epochs <span>=</span> <span>10</span>
log_steps <span>=</span> <span>20</span>  <span># Number of steps to print log</span>
model_dir <span>=</span> <span>'ckpt'</span> <span># Model checkpoint</span>
learning_rate <span>=</span> <span>0.01</span> <span># Learning rate</span>
optimizer <span>=</span> <span>'adam'</span> <span># Optimizer algorithm</span>
run_mode <span>=</span> <span>'train'</span> <span># Run mode</span>

euler_graph <span>=</span> tf_euler<span>.</span>dataset<span>.</span>get_dataset<span>(</span>dataset<span>)</span>
euler_graph<span>.</span>load_graph<span>(</span><span>)</span>

dims <span>=</span> <span>[</span>hidden_dim<span>]</span> <span>*</span> <span>(</span>layers <span>+</span> <span>1</span><span>)</span>

<span>if</span> run_mode <span>==</span> <span>'train'</span><span>:</span>
    metapath <span>=</span> <span>[</span>euler_graph<span>.</span>train_edge_type<span>]</span> <span>*</span> layers  <span># metapath = [['train'], ['train']]</span>
<span>else</span><span>:</span>
    metapath <span>=</span> <span>[</span>euler_graph<span>.</span>all_edge_type<span>]</span> <span>*</span> layers

num_steps <span>=</span> <span>int</span><span>(</span><span>(</span>euler_graph<span>.</span>total_size <span>+</span> <span>1</span><span>)</span> <span>//</span> batch_size <span>*</span> num_epochs<span>)</span>

model <span>=</span> SupervisedGraphSage<span>(</span>dims<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                            euler_graph<span>.</span>feature_idx<span>,</span>
                            euler_graph<span>.</span>feature_dim<span>,</span>
                            euler_graph<span>.</span>label_idx<span>,</span>
                            euler_graph<span>.</span>label_dim<span>)</span>

params <span>=</span> <span>{</span><span>'train_node_type'</span><span>:</span> euler_graph<span>.</span>train_node_type<span>[</span><span>0</span><span>]</span><span>,</span>
          <span>'batch_size'</span><span>:</span> batch_size<span>,</span>
          <span>'optimizer'</span><span>:</span> optimizer<span>,</span>
          <span>'learning_rate'</span><span>:</span> learning_rate<span>,</span>
          <span>'log_steps'</span><span>:</span> log_steps<span>,</span>
          <span>'model_dir'</span><span>:</span> model_dir<span>,</span>
          <span>'id_file'</span><span>:</span> euler_graph<span>.</span>id_file<span>,</span>
          <span>'infer_dir'</span><span>:</span> model_dir<span>,</span>
          <span>'total_step'</span><span>:</span> num_steps<span>}</span>

config <span>=</span> tf<span>.</span>estimator<span>.</span>RunConfig<span>(</span>log_step_count_steps<span>=</span><span>None</span><span>)</span>

model_estimator <span>=</span> NodeEstimator<span>(</span>model<span>,</span> params<span>,</span> config<span>)</span>

<span>if</span> run_mode <span>==</span> <span>'train'</span><span>:</span>
    model_estimator<span>.</span>train<span>(</span><span>)</span>
<span>elif</span> run_mode <span>==</span> <span>'evaluate'</span><span>:</span>
    model_estimator<span>.</span>evaluate<span>(</span><span>)</span>
<span>elif</span> run_mode <span>==</span> <span>'infer'</span><span>:</span>
    model_estimator<span>.</span>infer<span>(</span><span>)</span>
<span>else</span><span>:</span>
    <span>raise</span> ValueError<span>(</span><span>'Run mode not exist!'</span><span>)</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>传入的model为SupervisedGraphSage，定义如下：</p>
<div><pre><code><span>class</span> <span>SupervisedGraphSage</span><span>(</span>SuperviseModel<span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dims<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                 feature_idx<span>,</span> feature_dim<span>,</span>
                 label_idx<span>,</span> label_dim<span>,</span> max_id<span>=</span><span>-</span><span>1</span><span>)</span><span>:</span>
        <span>super</span><span>(</span>SupervisedGraphSage<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>label_idx<span>,</span>
                                                  label_dim<span>)</span>
        self<span>.</span>gnn <span>=</span> GNN<span>(</span><span>'sage'</span><span>,</span> <span>'sage'</span><span>,</span> dims<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                       feature_idx<span>,</span> feature_dim<span>,</span> max_id<span>=</span>max_id<span>)</span>

    <span>def</span> <span>embed</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        <span>return</span> self<span>.</span>gnn<span>(</span>n_id<span>)</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>其中，GNN为一个消息传递层，我们暂时不去关心，我们先来看一下他的父类tf_euler.python.mp_utils.base.SupervisedModel：</p>
<div><pre><code><span>class</span> <span>SuperviseModel</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> label_idx<span>,</span> label_dim<span>,</span> metric_name<span>=</span><span>'f1'</span><span>)</span><span>:</span>
        self<span>.</span>label_idx <span>=</span> label_idx
        self<span>.</span>label_dim <span>=</span> label_dim
        self<span>.</span>metric_name <span>=</span> metric_name
        self<span>.</span>metric_class <span>=</span> tf_euler<span>.</span>utils<span>.</span>metrics<span>.</span>get<span>(</span>metric_name<span>)</span>
        self<span>.</span>out_fc <span>=</span> tf<span>.</span>layers<span>.</span>Dense<span>(</span>label_dim<span>,</span> use_bias<span>=</span><span>False</span><span>)</span>

    <span>def</span> <span>embed</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        <span>raise</span> NotImplementedError

    <span>def</span> <span>__call__</span><span>(</span>self<span>,</span> inputs<span>)</span><span>:</span>
        label<span>,</span> <span>=</span> tf_euler<span>.</span>get_dense_feature<span>(</span>inputs<span>,</span>
                                            <span>[</span>self<span>.</span>label_idx<span>]</span><span>,</span>
                                            <span>[</span>self<span>.</span>label_dim<span>]</span><span>)</span>
        embedding <span>=</span> self<span>.</span>embed<span>(</span>inputs<span>)</span>
        logit <span>=</span> self<span>.</span>out_fc<span>(</span>embedding<span>)</span>

        metric <span>=</span> self<span>.</span>metric_class<span>(</span>
            label<span>,</span> tf<span>.</span>nn<span>.</span>sigmoid<span>(</span>logit<span>)</span><span>)</span>
        loss <span>=</span> tf<span>.</span>nn<span>.</span>sigmoid_cross_entropy_with_logits<span>(</span>
            labels<span>=</span>label<span>,</span> logits<span>=</span>logit<span>)</span>
        loss <span>=</span> tf<span>.</span>reduce_mean<span>(</span>loss<span>)</span>
        <span>return</span> <span>(</span>embedding<span>,</span> loss<span>,</span> self<span>.</span>metric_name<span>,</span> metric<span>)</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>该类具体做了如下工作：</p>
<ul>
<li>将得到的inputs（n_id）使用euler服务器获取节点对应的标签</li>
<li>使用embed()方法获取节点对应的embedding</li>
<li>通过out_fc（一个线性层）获取输出logit</li>
<li>计算metric和loss</li>
</ul>
<p>这里还是没有采样的逻辑，于是我们可以推断采样的流程在之前忽略的GNN层中，我们反过来看GNN层：</p>
<div><pre><code><span>from</span> tf_euler<span>.</span>python<span>.</span>mp_utils<span>.</span>base_gnn <span>import</span> BaseGNNNet
<span>from</span> tf_euler<span>.</span>python<span>.</span>mp_utils<span>.</span>base <span>import</span> SuperviseModel<span>,</span> UnsuperviseModel


<span>class</span> <span>GNN</span><span>(</span>BaseGNNNet<span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> conv<span>,</span> flow<span>,</span>
                 dims<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                 feature_idx<span>,</span> feature_dim<span>,</span>
                 add_self_loops<span>=</span><span>False</span><span>,</span>
                 max_id<span>=</span><span>-</span><span>1</span><span>,</span> <span>**</span>kwargs<span>)</span><span>:</span>
        <span>super</span><span>(</span>GNN<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>conv<span>=</span>conv<span>,</span>
                                  flow<span>=</span>flow<span>,</span>
                                  dims<span>=</span>dims<span>,</span>
                                  fanouts<span>=</span>fanouts<span>,</span>
                                  metapath<span>=</span>metapath<span>,</span>
                                  add_self_loops<span>=</span>add_self_loops<span>,</span>
                                  max_id<span>=</span>max_id<span>,</span>
                                  <span>**</span>kwargs<span>)</span>
        <span>if</span> <span>not</span> <span>isinstance</span><span>(</span>feature_idx<span>,</span> <span>list</span><span>)</span><span>:</span>
            feature_idx <span>=</span> <span>[</span>feature_idx<span>]</span>
        <span>if</span> <span>not</span> <span>isinstance</span><span>(</span>feature_dim<span>,</span> <span>list</span><span>)</span><span>:</span>
            feature_dim <span>=</span> <span>[</span>feature_dim<span>]</span>
        self<span>.</span>feature_idx <span>=</span> feature_idx
        self<span>.</span>feature_dim <span>=</span> feature_dim

    <span>def</span> <span>to_x</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        x<span>,</span> <span>=</span> tf_euler<span>.</span>get_dense_feature<span>(</span>n_id<span>,</span>
                                        self<span>.</span>feature_idx<span>,</span>
                                        self<span>.</span>feature_dim<span>)</span>
        <span>return</span> x
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>GNN层只是重写了一个 to_x() 方法，只是将输入的节点id使用euler服务器转换为节点对应的特征。</p>
<p>于是我们继续去看他的父类tf_euler.python.mp_utils.base_gnn.BaseGNNNet：</p>
<div><pre><code><span>class</span> <span>BaseGNNNet</span><span>(</span><span>object</span><span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> conv<span>,</span> flow<span>,</span> dims<span>,</span>
                 fanouts<span>,</span> metapath<span>,</span>
                 add_self_loops<span>=</span><span>True</span><span>,</span>
                 max_id<span>=</span><span>-</span><span>1</span><span>,</span>
                 <span>**</span>kwargs<span>)</span><span>:</span>
        conv_class <span>=</span> utils<span>.</span>get_conv_class<span>(</span>conv<span>)</span>
        flow_class <span>=</span> utils<span>.</span>get_flow_class<span>(</span>flow<span>)</span>
        <span>if</span> flow_class <span>==</span> <span>'whole'</span><span>:</span>
            self<span>.</span>whole_graph <span>=</span> <span>True</span>
        <span>else</span><span>:</span>
            self<span>.</span>whole_graph <span>=</span> <span>False</span>
        self<span>.</span>convs <span>=</span> <span>[</span><span>]</span>
        <span>for</span> dim <span>in</span> dims<span>[</span><span>:</span><span>-</span><span>1</span><span>]</span><span>:</span>
            self<span>.</span>convs<span>.</span>append<span>(</span>self<span>.</span>get_conv<span>(</span>conv_class<span>,</span> dim<span>)</span><span>)</span>
        self<span>.</span>fc <span>=</span> tf<span>.</span>layers<span>.</span>Dense<span>(</span>dims<span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
        self<span>.</span>sampler <span>=</span> flow_class<span>(</span>fanouts<span>,</span> metapath<span>,</span> add_self_loops<span>,</span> max_id<span>=</span>max_id<span>)</span>

    <span>def</span> <span>get_conv</span><span>(</span>self<span>,</span> conv_class<span>,</span> dim<span>)</span><span>:</span>
        <span>return</span> conv_class<span>(</span>dim<span>)</span>

    <span>def</span> <span>to_x</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        <span>raise</span> NotImplementedError

    <span>def</span> <span>to_edge</span><span>(</span>self<span>,</span> n_id_src<span>,</span> n_id_dst<span>,</span> e_id<span>)</span><span>:</span>
        <span>return</span> e_id

    <span>def</span> <span>get_edge_attr</span><span>(</span>self<span>,</span> block<span>)</span><span>:</span>
        n_id_dst <span>=</span> tf<span>.</span>cast<span>(</span>tf<span>.</span>expand_dims<span>(</span>block<span>.</span>n_id<span>,</span> <span>-</span><span>1</span><span>)</span><span>,</span>
                           dtype<span>=</span>tf<span>.</span>float32<span>)</span>
        n_id_src<span>=</span> mp_ops<span>.</span>gather<span>(</span>n_id_dst<span>,</span> block<span>.</span>res_n_id<span>)</span>
        n_id_src <span>=</span> mp_ops<span>.</span>gather<span>(</span>n_id_src<span>,</span>
                                 block<span>.</span>edge_index<span>[</span><span>0</span><span>]</span><span>)</span>
        n_id_dst <span>=</span> mp_ops<span>.</span>gather<span>(</span>n_id_dst<span>,</span>
                                 block<span>.</span>edge_index<span>[</span><span>1</span><span>]</span><span>)</span>
        n_id_src <span>=</span> tf<span>.</span>cast<span>(</span>tf<span>.</span>squeeze<span>(</span>n_id_src<span>,</span> <span>-</span><span>1</span><span>)</span><span>,</span> dtype<span>=</span>tf<span>.</span>int64<span>)</span>
        n_id_dst <span>=</span> tf<span>.</span>cast<span>(</span>tf<span>.</span>squeeze<span>(</span>n_id_dst<span>,</span> <span>-</span><span>1</span><span>)</span><span>,</span> dtype<span>=</span>tf<span>.</span>int64<span>)</span>
        edge_attr <span>=</span> self<span>.</span>to_edge<span>(</span>n_id_src<span>,</span> n_id_dst<span>,</span> block<span>.</span>e_id<span>)</span>
        <span>return</span> edge_attr



    <span>def</span> <span>calculate_conv</span><span>(</span>self<span>,</span> conv<span>,</span> inputs<span>,</span> edge_index<span>,</span>
                       size<span>=</span><span>None</span><span>,</span> edge_attr<span>=</span><span>None</span><span>)</span><span>:</span>
        <span>return</span> conv<span>(</span>inputs<span>,</span> edge_index<span>,</span> size<span>=</span>size<span>,</span> edge_attr<span>=</span>edge_attr<span>)</span>

    <span>def</span> <span>__call__</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        data_flow <span>=</span> self<span>.</span>sampler<span>(</span>n_id<span>)</span>
        num_layers <span>=</span> <span>len</span><span>(</span>self<span>.</span>convs<span>)</span>
        x <span>=</span> self<span>.</span>to_x<span>(</span>data_flow<span>[</span><span>0</span><span>]</span><span>.</span>n_id<span>)</span>
        <span>for</span> i<span>,</span> conv<span>,</span> block <span>in</span> <span>zip</span><span>(</span><span>range</span><span>(</span>num_layers<span>)</span><span>,</span> self<span>.</span>convs<span>,</span> data_flow<span>)</span><span>:</span>
            <span>if</span> block<span>.</span>e_id <span>is</span> <span>None</span><span>:</span>
                edge_attr <span>=</span> <span>None</span>
            <span>else</span><span>:</span>
                edge_attr <span>=</span> self<span>.</span>get_edge_attr<span>(</span>block<span>)</span>
            x_src <span>=</span> mp_ops<span>.</span>gather<span>(</span>x<span>,</span> block<span>.</span>res_n_id<span>)</span>
            x_dst <span>=</span> <span>None</span> <span>if</span> self<span>.</span>whole_graph <span>else</span> x
            x <span>=</span> self<span>.</span>calculate_conv<span>(</span>conv<span>,</span>
                                    <span>(</span>x_src<span>,</span> x_dst<span>)</span><span>,</span>
                                    block<span>.</span>edge_index<span>,</span>
                                    size<span>=</span>block<span>.</span>size<span>,</span>
                                    edge_attr<span>=</span>edge_attr<span>)</span>
            x <span>=</span> tf<span>.</span>nn<span>.</span>relu<span>(</span>x<span>)</span>
        x <span>=</span> self<span>.</span>fc<span>(</span>x<span>)</span>
        <span>return</span> x
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>初始化时，他获取了两个类对象：</p>
<ul>
<li>conv_class：卷积汇聚方法，这里是：tf_euler.python.convolution.SAGEConv</li>
<li>flow_class：图抽样方法，这里是：tf_euler.python.dataflow.SageDataFlow</li>
</ul>
<h3 id="图抽样方法-构造消息传递的路径——dataflow" tabindex="-1"> 图抽样方法：构造消息传递的路径——DataFlow</h3>
<p>使用flow_class创建了一个采样器sampler，我们来看一下这个sampler的定义：</p>
<div><pre><code><span>from</span> tf_euler<span>.</span>python<span>.</span>dataflow<span>.</span>neighbor_dataflow <span>import</span> UniqueDataFlow

<span>class</span> <span>SageDataFlow</span><span>(</span>UniqueDataFlow<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                 add_self_loops<span>=</span><span>True</span><span>,</span>
                 max_id<span>=</span><span>-</span><span>1</span><span>,</span>
                 <span>**</span>kwargs<span>)</span><span>:</span>
        <span>super</span><span>(</span>SageDataFlow<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>num_hops<span>=</span><span>len</span><span>(</span>metapath<span>)</span><span>,</span>
                                           add_self_loops<span>=</span>add_self_loops<span>)</span>
        self<span>.</span>fanouts <span>=</span> fanouts
        self<span>.</span>metapath <span>=</span> metapath
        self<span>.</span>max_id <span>=</span> max_id

    <span>def</span> <span>get_neighbors</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        neighbors <span>=</span> <span>[</span><span>]</span>
        neighbor_src <span>=</span> <span>[</span><span>]</span>
        <span>for</span> hop_edge_types<span>,</span> count <span>in</span> <span>zip</span><span>(</span>self<span>.</span>metapath<span>,</span> self<span>.</span>fanouts<span>)</span><span>:</span>
            n_id <span>=</span> tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            one_neighbor<span>,</span> _w<span>,</span> _ <span>=</span> tf_euler<span>.</span>sample_neighbor<span>(</span>
                n_id<span>,</span> hop_edge_types<span>,</span> count<span>,</span> default_node<span>=</span>self<span>.</span>max_id<span>+</span><span>1</span><span>)</span>
            neighbors<span>.</span>append<span>(</span>tf<span>.</span>reshape<span>(</span>one_neighbor<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span><span>)</span>
            node_src <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>)</span>
            node_src <span>=</span> tf<span>.</span>tile<span>(</span>tf<span>.</span>reshape<span>(</span>node_src<span>,</span> <span>[</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>]</span><span>)</span><span>,</span> <span>[</span><span>1</span><span>,</span> count<span>]</span><span>)</span>
            node_src <span>=</span> tf<span>.</span>reshape<span>(</span>node_src<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            neighbor_src<span>.</span>append<span>(</span>node_src<span>)</span>
            new_n_id <span>=</span> tf<span>.</span>reshape<span>(</span>one_neighbor<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            n_id <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>new_n_id<span>,</span> n_id<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
            n_id<span>,</span> _ <span>=</span> tf<span>.</span>unique<span>(</span>tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span><span>)</span>
        <span>return</span> neighbors<span>,</span> neighbor_src
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>单看这个类看不出什么，我们需要找到调用他的__call__()方法，在父类中找：</p>
<div><pre><code><span>class</span> <span>UniqueDataFlow</span><span>(</span>NeighborDataFlow<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> num_hops<span>,</span>
                 add_self_loops<span>=</span><span>True</span><span>)</span><span>:</span>
        <span>super</span><span>(</span>UniqueDataFlow<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>num_hops<span>=</span>num_hops<span>,</span>
                                             add_self_loops<span>=</span>add_self_loops<span>)</span>

    <span>def</span> <span>produce_subgraph</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        n_id <span>=</span> tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
        inv <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>)</span>
        last_idx <span>=</span> inv

        data_flow <span>=</span> DataFlow<span>(</span>n_id<span>)</span>
        n_neighbors<span>,</span> n_edge_src <span>=</span> self<span>.</span>get_neighbors<span>(</span>n_id<span>)</span>
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_hops<span>)</span><span>:</span>
            new_n_id <span>=</span> n_neighbors<span>[</span>i<span>]</span>
            edge_src <span>=</span> n_edge_src<span>[</span>i<span>]</span>

            new_n_id <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>new_n_id<span>,</span> n_id<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
            new_n_id<span>,</span> new_inv <span>=</span> tf<span>.</span>unique<span>(</span>new_n_id<span>)</span>
            res_n_id <span>=</span> new_inv<span>[</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>:</span><span>]</span>
            <span>if</span> self<span>.</span>add_self_loops<span>:</span>
                edge_src <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>edge_src<span>,</span> last_idx<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
                last_idx <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>new_n_id<span>)</span><span>)</span>
            <span>else</span><span>:</span>
                new_inv <span>=</span> new_inv<span>[</span><span>:</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>]</span>
                last_idx <span>=</span> new_inv

            n_id <span>=</span> new_n_id
            edge_dst <span>=</span> new_inv
            edge_index <span>=</span> tf<span>.</span>stack<span>(</span><span>[</span>edge_src<span>,</span> edge_dst<span>]</span><span>,</span> <span>0</span><span>)</span>
            e_id <span>=</span> <span>None</span>
            data_flow<span>.</span>append<span>(</span>new_n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>)</span>
        <span>return</span> data_flow
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>同样没有__call__()方法，继续去父类找：</p>
<div><pre><code><span>from</span> tf_euler<span>.</span>python<span>.</span>dataflow<span>.</span>base_dataflow <span>import</span> DataFlow


<span>class</span> <span>NeighborDataFlow</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> num_hops<span>,</span>
                 add_self_loops<span>=</span><span>True</span><span>,</span>
                 <span>**</span>kwargs<span>)</span><span>:</span>
        self<span>.</span>num_hops <span>=</span> num_hops
        self<span>.</span>add_self_loops <span>=</span> add_self_loops

    <span>def</span> <span>get_neighbors</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        <span>'''
        The neighbor sampler in a mini-batch of n_id.
        It returns: neighbors: a list of 'tensor';
                    neighbor_src: a list of 'tensor'
        Input:
          n_id: input nodes
        Output:
          neighbors: [[n_id's neighbor], [n_id's neighbor's neighbor], ...]
          neighbor_src: [[n_neighbor_src], [n_neighbor_neighbor_src], ...]
        '''</span>
        <span>raise</span> NotImplementedError<span>(</span><span>)</span>

    <span>def</span> <span>produce_subgraph</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>  <span># 生成计算图（多个二分图）</span>
        n_id <span>=</span> tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
        inv <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>)</span>
        last_idx <span>=</span> inv

        data_flow <span>=</span> DataFlow<span>(</span>n_id<span>)</span>
        n_neighbors<span>,</span> n_edge_src <span>=</span> self<span>.</span>get_neighbors<span>(</span>n_id<span>)</span>
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_hops<span>)</span><span>:</span>
            new_n_id <span>=</span> n_neighbors<span>[</span>i<span>]</span>
            edge_src <span>=</span> n_edge_src<span>[</span>i<span>]</span>

            new_n_id <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>new_n_id<span>,</span> n_id<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
            new_inv <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>new_n_id<span>)</span><span>)</span>
            res_n_id <span>=</span> new_inv<span>[</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>:</span><span>]</span>
            <span>if</span> self<span>.</span>add_self_loops<span>:</span>
                edge_src <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>edge_src<span>,</span> last_idx<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
                last_idx <span>=</span> new_inv
            <span>else</span><span>:</span>
                new_inv <span>=</span> new_inv<span>[</span><span>:</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>]</span>
                last_idx <span>=</span> new_inv

            n_id <span>=</span> new_n_id
            edge_dst <span>=</span> new_inv
            edge_index <span>=</span> tf<span>.</span>stack<span>(</span><span>[</span>edge_src<span>,</span> edge_dst<span>]</span><span>,</span> <span>0</span><span>)</span>
            e_id <span>=</span> <span>None</span>
            data_flow<span>.</span>append<span>(</span>new_n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>)</span>
        <span>return</span> data_flow

    <span>def</span> <span>__call__</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        producer <span>=</span> self<span>.</span>produce_subgraph
        <span>return</span> producer<span>(</span>n_id<span>)</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>这次终于有了。。具体来说就是传入n_id需要采样的节点id，调用produce_subgraph()方法来获取消息传递用的计算图。该方法的目的为创建一个消息传递的DataFlow，DataFlow是一个Block的列表，以下是两者的定义：</p>
<div><pre><code><span>class</span> <span>Block</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>,</span> size<span>)</span><span>:</span>
        self<span>.</span>n_id <span>=</span> n_id  <span># 二分图中消息传递的起点</span>
        self<span>.</span>res_n_id <span>=</span> res_n_id  <span># 二分图中消息传递的终点</span>
        self<span>.</span>e_id <span>=</span> e_id  <span># edge_index中边的id</span>
        self<span>.</span>edge_index <span>=</span> edge_index  <span># 二分图的边，[2, edge_sizes]</span>
        self<span>.</span>size <span>=</span> size  <span># 二分图的形状 (M, N)</span>


<span>class</span> <span>DataFlow</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        self<span>.</span>n_id <span>=</span> n_id
        self<span>.</span>__last_n_id__ <span>=</span> n_id
        self<span>.</span>blocks <span>=</span> <span>[</span><span>]</span>

    <span>def</span> <span>append</span><span>(</span>self<span>,</span> n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>)</span><span>:</span>
        size <span>=</span> <span>[</span>tf<span>.</span>shape<span>(</span>self<span>.</span>__last_n_id__<span>)</span><span>[</span><span>0</span><span>]</span><span>,</span> tf<span>.</span>shape<span>(</span>n_id<span>)</span><span>[</span><span>0</span><span>]</span><span>]</span>
        block <span>=</span> Block<span>(</span>n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>,</span> size<span>)</span>
        self<span>.</span>blocks<span>.</span>append<span>(</span>block<span>)</span>
        self<span>.</span>__last_n_id__ <span>=</span> n_id

    <span>def</span> <span>__len__</span><span>(</span>self<span>)</span><span>:</span>
        <span>return</span> <span>len</span><span>(</span>self<span>.</span>blocks<span>)</span>

    <span>def</span> <span>__getitem__</span><span>(</span>self<span>,</span> idx<span>)</span><span>:</span>
        <span>return</span> self<span>.</span>blocks<span>[</span><span>:</span><span>:</span><span>-</span><span>1</span><span>]</span><span>[</span>idx<span>]</span>

    <span>def</span> <span>__iter__</span><span>(</span>self<span>)</span><span>:</span>
        <span>for</span> block <span>in</span> self<span>.</span>blocks<span>[</span><span>:</span><span>:</span><span>-</span><span>1</span><span>]</span><span>:</span>
            <span>yield</span> block
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><ul>
<li>Block是一个表示消息传递的二分图，二分图两边的节点分别为n_id 与 res_n_id，n_id，消息将会从 n_id 表示的节点传递到 res_n_id 表示的节点。</li>
<li>Block的形状定义成(M, N)， M为src, N为dst，但在Euler中，消息是从dst传递给src的（这点与PyG的实现不同）。</li>
<li>n_id 为节点原本的id，而res_n_id使用的是n_id数组的索引。（这个在后面的代码中会有体现）</li>
<li>DataFlow是多个Block的列表，表示消息从第k阶邻居一级一级传递到目标节点的全部过程。每传递一级都使用一个二分图Block表示其经过的节点。</li>
<li>由于邻居采样的过程与消息传递的方向是相反的，所以可以看到DataFlow的__getitem__与__iter__方法中都是从后往前遍历的。</li>
</ul>
<p>UniqueDataFlow重写了NeighborDataFlow中的produce_subgraph()方法，于是我们直接来看UniqueDataFlow类的produce_subgraph()方法，由于下面的代码中有两种节点编号方式，容易混乱，我直接在代码中逐行加上注释，便于理解：</p>
<div><pre><code><span># 注：Euler框架中，边的方向指向的为邻居采样的方向，源节点（src）的邻居为其边指向的其他节点（dst），与消息传递的方向相反的。</span>
<span># 注：以下用词中，"节点编号"表示节点全局的编号，"索引"表示n_id数组的下标</span>
<span>def</span> <span>produce_subgraph</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
    n_id <span>=</span> tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>  <span># n_id 为需要采样的源节点编号</span>
    inv <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>)</span>  <span># inv 为 n_id的索引</span>
    last_idx <span>=</span> inv  <span># 保留最后一次的索引，即源节点的索引</span>

    data_flow <span>=</span> DataFlow<span>(</span>n_id<span>)</span>  <span># 创建空的DataFlow，将n_id作为邻居采样的起点</span>
    n_neighbors<span>,</span> n_edge_src <span>=</span> self<span>.</span>get_neighbors<span>(</span>n_id<span>)</span>  <span># 获取k阶的邻居</span>
    <span># neighbors：一个列表：[[n_id的邻居], [n_id的邻居的邻居], …]</span>
    <span># n_edge_src：一个列表：[[n_neighbor_src], [n_neighbor_neighbor_src], …]，表示neighbors中邻居对应的源节点索引</span>
    <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_hops<span>)</span><span>:</span>
        new_n_id <span>=</span> n_neighbors<span>[</span>i<span>]</span> <span># new_n_id表示本轮邻居节点编号（dst）</span>
        edge_src <span>=</span> n_edge_src<span>[</span>i<span>]</span>  <span># edge_src表示本轮邻居的源节点索引（src）</span>

        new_n_id <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>new_n_id<span>,</span> n_id<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
        new_n_id<span>,</span> new_inv <span>=</span> tf<span>.</span>unique<span>(</span>new_n_id<span>)</span> <span># 将邻居节点与源节点编号取一个并集，并获得新节点的索引（作为边的终点）</span>
        res_n_id <span>=</span> new_inv<span>[</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>:</span><span>]</span>  <span># res_n_id为n_id的新索引，即源节点在下一级节点列表中的索引</span>
        
        <span># 下面这块主要是为了更新last_idx，分为两种情况</span>
        <span>if</span> self<span>.</span>add_self_loops<span>:</span>  <span># 添加自环边</span>
            edge_src <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>edge_src<span>,</span> last_idx<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span> <span># 将源节点的索引添加到邻居的源节点索引后面，作为边的源节点</span>
            last_idx <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>new_n_id<span>)</span><span>)</span> <span># 保存源节点索引</span>
        <span>else</span><span>:</span>  <span># 不添加自环边</span>
            new_inv <span>=</span> new_inv<span>[</span><span>:</span><span>-</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>]</span> <span># 不添加自环边的话，把之前加入的多余索引去除掉</span>
            last_idx <span>=</span> new_inv

        n_id <span>=</span> new_n_id
        edge_dst <span>=</span> new_inv
        edge_index <span>=</span> tf<span>.</span>stack<span>(</span><span>[</span>edge_src<span>,</span> edge_dst<span>]</span><span>,</span> <span>0</span><span>)</span>
        e_id <span>=</span> <span>None</span>
        data_flow<span>.</span>append<span>(</span>new_n_id<span>,</span> res_n_id<span>,</span> e_id<span>,</span> edge_index<span>)</span>
    <span>return</span> data_flow
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/388FCB1A-9845-4F79-9ED4-97E689DD8ED0/14D5F7CE-B0DB-4ED5-8194-28A4F9F94F38_2/IMG_8059.jpeg" alt="IMG_8059.jpeg" loading="lazy"></p>
<p>关于两种不同的方向</p>
<p>再来看一下get_neighbors(n_id)的具体实现：</p>
<div><pre><code><span>from</span> tf_euler<span>.</span>python<span>.</span>dataflow<span>.</span>neighbor_dataflow <span>import</span> UniqueDataFlow

<span>class</span> <span>SageDataFlow</span><span>(</span>UniqueDataFlow<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> fanouts<span>,</span> metapath<span>,</span>
                 add_self_loops<span>=</span><span>True</span><span>,</span>
                 max_id<span>=</span><span>-</span><span>1</span><span>,</span>
                 <span>**</span>kwargs<span>)</span><span>:</span>
        <span>super</span><span>(</span>SageDataFlow<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>num_hops<span>=</span><span>len</span><span>(</span>metapath<span>)</span><span>,</span>
                                           add_self_loops<span>=</span>add_self_loops<span>)</span>
        self<span>.</span>fanouts <span>=</span> fanouts
        self<span>.</span>metapath <span>=</span> metapath
        self<span>.</span>max_id <span>=</span> max_id

    <span>def</span> <span>get_neighbors</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        neighbors <span>=</span> <span>[</span><span>]</span>
        neighbor_src <span>=</span> <span>[</span><span>]</span>
        <span>for</span> hop_edge_types<span>,</span> count <span>in</span> <span>zip</span><span>(</span>self<span>.</span>metapath<span>,</span> self<span>.</span>fanouts<span>)</span><span>:</span>
            n_id <span>=</span> tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            one_neighbor<span>,</span> _w<span>,</span> _ <span>=</span> tf_euler<span>.</span>sample_neighbor<span>(</span>
                n_id<span>,</span> hop_edge_types<span>,</span> count<span>,</span> default_node<span>=</span>self<span>.</span>max_id<span>+</span><span>1</span><span>)</span>
            neighbors<span>.</span>append<span>(</span>tf<span>.</span>reshape<span>(</span>one_neighbor<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span><span>)</span>
            node_src <span>=</span> tf<span>.</span><span>range</span><span>(</span>tf<span>.</span>size<span>(</span>n_id<span>)</span><span>)</span>
            node_src <span>=</span> tf<span>.</span>tile<span>(</span>tf<span>.</span>reshape<span>(</span>node_src<span>,</span> <span>[</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>]</span><span>)</span><span>,</span> <span>[</span><span>1</span><span>,</span> count<span>]</span><span>)</span>
            node_src <span>=</span> tf<span>.</span>reshape<span>(</span>node_src<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            neighbor_src<span>.</span>append<span>(</span>node_src<span>)</span>
            new_n_id <span>=</span> tf<span>.</span>reshape<span>(</span>one_neighbor<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>
            n_id <span>=</span> tf<span>.</span>concat<span>(</span><span>[</span>new_n_id<span>,</span> n_id<span>]</span><span>,</span> axis<span>=</span><span>0</span><span>)</span>
            n_id<span>,</span> _ <span>=</span> tf<span>.</span>unique<span>(</span>tf<span>.</span>reshape<span>(</span>n_id<span>,</span> <span>[</span><span>-</span><span>1</span><span>]</span><span>)</span><span>)</span>
        <span>return</span> neighbors<span>,</span> neighbor_src
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><ul>
<li>调用get_neighbors(n_id)对节点进行邻居采样，得到的结果为两个列表：
<ul>
<li>neighbors: [[n_id的邻居], [n_id的邻居的邻居], …]</li>
<li>neighbor_src: [[n_neighbor_src], [n_neighbor_neighbor_src], …]</li>
<li>具体例子如下图，当n_id=[1,2,3]时，得到下图的结果：</li>
</ul>
</li>
</ul>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/388FCB1A-9845-4F79-9ED4-97E689DD8ED0/6965899C-7D53-471A-A1CF-167280644C82_2/Image.png" alt="Image.png" loading="lazy"></p>
<ul>
<li>从源节点开始，调用tf_euler.sample_neighbor方法采样源节点的第一阶邻居，并且会根据metapath，以及fanouts，来确定要采样的邻居的类型以及数量。之后再循环，直到采样完k阶邻居。</li>
</ul>
<p>至此，我们介绍完了消息传递路径是如何构造的，接下来，看一下消息是如何聚合的，也就是图卷积模块。</p>
<h3 id="卷积汇聚方法-将邻居消息进行汇聚——sageconv" tabindex="-1"> 卷积汇聚方法：将邻居消息进行汇聚——SageConv</h3>
<p>回来看一下BaseGNNNet的__call__方法：</p>
<div><pre><code><span>class</span> <span>BaseGNNNet</span><span>(</span><span>object</span><span>)</span><span>:</span>
    <span>.</span><span>.</span><span>.</span>
    <span>def</span> <span>__call__</span><span>(</span>self<span>,</span> n_id<span>)</span><span>:</span>
        data_flow <span>=</span> self<span>.</span>sampler<span>(</span>n_id<span>)</span>
        num_layers <span>=</span> <span>len</span><span>(</span>self<span>.</span>convs<span>)</span>
        x <span>=</span> self<span>.</span>to_x<span>(</span>data_flow<span>[</span><span>0</span><span>]</span><span>.</span>n_id<span>)</span>
        <span>for</span> i<span>,</span> conv<span>,</span> block <span>in</span> <span>zip</span><span>(</span><span>range</span><span>(</span>num_layers<span>)</span><span>,</span> self<span>.</span>convs<span>,</span> data_flow<span>)</span><span>:</span>
            <span>if</span> block<span>.</span>e_id <span>is</span> <span>None</span><span>:</span>
                edge_attr <span>=</span> <span>None</span>
            <span>else</span><span>:</span>
                edge_attr <span>=</span> self<span>.</span>get_edge_attr<span>(</span>block<span>)</span>
            x_src <span>=</span> mp_ops<span>.</span>gather<span>(</span>x<span>,</span> block<span>.</span>res_n_id<span>)</span>
            x_dst <span>=</span> <span>None</span> <span>if</span> self<span>.</span>whole_graph <span>else</span> x
            x <span>=</span> self<span>.</span>calculate_conv<span>(</span>conv<span>,</span>
                                    <span>(</span>x_src<span>,</span> x_dst<span>)</span><span>,</span>
                                    block<span>.</span>edge_index<span>,</span>
                                    size<span>=</span>block<span>.</span>size<span>,</span>
                                    edge_attr<span>=</span>edge_attr<span>)</span>
            x <span>=</span> tf<span>.</span>nn<span>.</span>relu<span>(</span>x<span>)</span>
        x <span>=</span> self<span>.</span>fc<span>(</span>x<span>)</span>
        <span>return</span> x
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><ul>
<li>获取消息传递路径DataFlow：上文提及的表示消息传递路径的列表，由多个二分图构成。</li>
<li>获取dst节点（消息的发出方）的特征：x</li>
<li>进行循环：
<ul>
<li>获取边的属性：edge_attr</li>
<li>获取src节点（消息的接收方）的特征：x_src</li>
<li>使用定义的conv模块进行卷积计算，完成一层的消息传递</li>
<li>对卷积完的隐藏特征使用非线形激活函数</li>
</ul>
</li>
<li>进行完k次循环后，消息已经从k阶邻居汇聚到源节点</li>
<li>再使用线形层对特征进行最后一轮映射</li>
</ul>
<p>这里主要是由一个conv模块完成的卷积操作，我们使用SageConv作为例子继续深入研究：</p>
<div><pre><code><span>class</span> <span>SAGEConv</span><span>(</span>conv<span>.</span>Conv<span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> <span>**</span>kwargs<span>)</span><span>:</span>
        <span>super</span><span>(</span>SAGEConv<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>aggr<span>=</span><span>'mean'</span><span>,</span> <span>**</span>kwargs<span>)</span>
        self<span>.</span>self_fc <span>=</span> tf<span>.</span>layers<span>.</span>Dense<span>(</span>dim<span>,</span> use_bias<span>=</span><span>False</span><span>)</span>
        self<span>.</span>neigh_fc <span>=</span> tf<span>.</span>layers<span>.</span>Dense<span>(</span>dim<span>,</span> use_bias<span>=</span><span>False</span><span>)</span>

    <span>def</span> <span>__call__</span><span>(</span>self<span>,</span> x<span>,</span> edge_index<span>,</span> size<span>=</span><span>None</span><span>,</span> <span>**</span>kwargs<span>)</span><span>:</span>
        gather_x<span>,</span> <span>=</span> self<span>.</span>gather_feature<span>(</span><span>[</span>x<span>]</span><span>,</span> edge_index<span>)</span>
        out <span>=</span> self<span>.</span>apply_edge<span>(</span>gather_x<span>[</span><span>1</span><span>]</span><span>)</span>
        out <span>=</span> mp_ops<span>.</span>scatter_<span>(</span>self<span>.</span>aggr<span>,</span> out<span>,</span> edge_index<span>[</span><span>0</span><span>]</span><span>,</span> size<span>=</span>size<span>[</span><span>0</span><span>]</span><span>)</span>
        out <span>=</span> self<span>.</span>apply_node<span>(</span>out<span>,</span> x<span>[</span><span>0</span><span>]</span><span>)</span>
        <span>return</span> out

    <span>def</span> <span>apply_edge</span><span>(</span>self<span>,</span> x_j<span>)</span><span>:</span>
        <span>return</span> x_j

    <span>def</span> <span>apply_node</span><span>(</span>self<span>,</span> aggr_out<span>,</span> x<span>)</span><span>:</span>
        <span>return</span> self<span>.</span>self_fc<span>(</span>x<span>)</span> <span>+</span> self<span>.</span>neigh_fc<span>(</span>aggr_out<span>)</span>
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>重点在__call__方法，首先调用了一个gather_feature的方法，该方法在其父类conv.Conv中进行了定义，该方法实际上就是将二分图两边的特征，即源节点的特征与目标节点的特征，细节不作介绍，具体代码如下：</p>
<div><pre><code><span>class</span> <span>Conv</span><span>(</span><span>object</span><span>)</span><span>:</span>

    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> aggr<span>=</span><span>'add'</span><span>)</span><span>:</span>
        self<span>.</span>aggr <span>=</span> aggr
        <span>assert</span> self<span>.</span>aggr <span>in</span> <span>[</span><span>'add'</span><span>,</span> <span>'mean'</span><span>,</span> <span>'max'</span><span>]</span>

    <span>def</span> <span>gather_feature</span><span>(</span>self<span>,</span> features<span>,</span> edge_index<span>)</span><span>:</span>
        convert_features <span>=</span> <span>[</span><span>]</span>

        <span>for</span> feature <span>in</span> features<span>:</span>
            convert_feature <span>=</span> <span>[</span><span>]</span>
            <span>assert</span> <span>isinstance</span><span>(</span>feature<span>,</span> <span>tuple</span><span>)</span> <span>or</span> <span>isinstance</span><span>(</span>feature<span>,</span> <span>list</span><span>)</span>
            <span>assert</span> <span>len</span><span>(</span>feature<span>)</span> <span>==</span> <span>2</span>
            <span>if</span> feature<span>[</span><span>1</span><span>]</span> <span>is</span> <span>None</span><span>:</span>
                feature<span>[</span><span>1</span><span>]</span> <span>=</span> feature<span>[</span><span>0</span><span>]</span>
            <span>for</span> idx<span>,</span> tmp <span>in</span> <span>enumerate</span><span>(</span>feature<span>)</span><span>:</span>
                <span>if</span> tmp <span>is</span> <span>not</span> <span>None</span><span>:</span>
                    tmp <span>=</span> mp_ops<span>.</span>gather<span>(</span>tmp<span>,</span> edge_index<span>[</span>idx<span>]</span><span>)</span>
                convert_feature<span>.</span>append<span>(</span>tmp<span>)</span>
            convert_features<span>.</span>append<span>(</span>convert_feature<span>)</span>
        <span>return</span> convert_features

    <span>def</span> <span>apply_edge</span><span>(</span>self<span>,</span> x_j<span>)</span><span>:</span>
        <span>return</span> x_j

    <span>def</span> <span>apply_node</span><span>(</span>self<span>,</span> aggr_out<span>)</span><span>:</span>
        <span>return</span> aggr_out
</code></pre><div aria-hidden="true"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><p>注意，Euler的消息传递方向是dst到src，所以在SAGEConv的__call__方法中，先获取了dst的特征gather_x[1]，再使用mp_ops.scatter_方法，再SAGEConv里面具体会调用mp_ops.scatter_mean方法，该方法会将输入的特征矩阵out，按照edge_index[0]的索引进行聚合，再对聚合后的每一堆行向量求平均。其实这也就是将邻居出现的所有特征聚合到一起（加在一起）再求个平均，得到了聚合后的结果。</p>
<p><img src="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/388FCB1A-9845-4F79-9ED4-97E689DD8ED0/6ADCD963-8D09-43F7-AFC2-062C801367C0_2/Image.jpeg" alt="Image.jpeg" loading="lazy"></p>
<p>scatter_mean方法将特征按照索引进行聚合</p>
<p>至此，我们就看完了图卷积模块的具体实现。</p>
<p>对于用户来说，其实不需要关注一个模块的内部是如何实现的，直接调用外部接口完成需要的任务就行，但是处于学习与研究的目的，还是选择看一看源码，一是可以学习代码具体实现，二是在使用的时候也更有把握一点，遇到问题也更容易快速定位。</p>
<p>芜湖，总算把这个笔记写完了。。。第一次写读源码的笔记，还真不容易。</p>
]]></content:encoded>
      <enclosure url="https://res.craft.do/user/full/4725d35f-0536-9d85-0a69-346665ba7ebe/doc/388FCB1A-9845-4F79-9ED4-97E689DD8ED0/14D5F7CE-B0DB-4ED5-8194-28A4F9F94F38_2/IMG_8059.jpeg" type="image/jpeg"/>
    </item>
  </channel>
</rss>